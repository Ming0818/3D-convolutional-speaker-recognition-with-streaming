{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# xs = list()\n",
    "# ys = list()\n",
    "# seqs =list()\n",
    "\n",
    "# for i, fname in enumerate(glob(\"data_array/*.h5\")):\n",
    "#     print \n",
    "#     h5f = h5py.File(fname, 'r')\n",
    "#     xs.append(h5f['utterances'][:].astype(np.float32))\n",
    "#     ys.append(h5f['labels'][:])\n",
    "#     seqs.append(h5f['seqlen'][:])\n",
    "#     h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xs = list()\n",
    "ys = list()\n",
    "seqs =list()\n",
    "\n",
    "for i, fname in enumerate(glob(\"data_array/*_0.h5\")):\n",
    "    print \n",
    "    h5f = h5py.File(fname, 'r')\n",
    "    xs.append(h5f['utterances'][:].astype(np.float32))\n",
    "    ys.append(h5f['labels'][:])\n",
    "    seqs.append(h5f['seqlen'][:])\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs= np.concatenate(xs, axis=0).reshape(-1,30,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys= np.concatenate(ys, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs= np.concatenate(seqs, axis=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3311, 30, 800)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3311, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3311,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train, xs_test, ys_train, ys_test, seqs_train, seqs_test = train_test_split(xs, ys, seqs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xs, ys, seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2218, 30, 800)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((xs_train, ys_train, seqs_train)).shuffle(buffer_size=10000).batch(32)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((xs_test, ys_test, seqs_test)).shuffle(buffer_size=10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper parameters\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "class DVectorNet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, out_dim, checkpoint_directory, batch_size=32, device_name=\"cpu:0\"):\n",
    "        super(DVectorNet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.batch_size = batch_size\n",
    "        self.device_name = device_name\n",
    "\n",
    "        # lstm cell\n",
    "        self.rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(64)\n",
    "\n",
    "        # cnn\n",
    "        # self.conv1 = tf.layers.Conv2D(32, 8, 8, padding='same', activation=tf.nn.relu)\n",
    "        # self.batch1 = tf.layers.BatchNormalization()\n",
    "        # self.conv2 = tf.layers.Conv2D(64, 4, 4, padding='same', activation=tf.nn.relu)\n",
    "        # self.batch2 = tf.layers.BatchNormalization()\n",
    "        # self.conv3 = tf.layers.Conv2D(64, 3, 3, padding='same', activation=tf.nn.relu)\n",
    "        # self.flatten = tf.layers.Flatten()\n",
    "\n",
    "        # dense\n",
    "        self.dense1 = tf.layers.Dense(512, activation=tf.nn.relu)\n",
    "        self.batch1 = tf.layers.BatchNormalization()\n",
    "        self.dense2 = tf.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.batch2 = tf.layers.BatchNormalization()\n",
    "        self.dense3 = tf.layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.batch3 = tf.layers.BatchNormalization()\n",
    "        self.dense4 = tf.layers.Dense(out_dim, activation=tf.nn.softmax)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "\n",
    "    def predict(self, X, seq_length, verbose=0, steps=None, training=False):\n",
    "        # Get the number of samples within a batch\n",
    "        num_samples = tf.shape(X)[0]\n",
    "\n",
    "        # Initialize LSTM cell state with zeros\n",
    "        state = self.rnn_cell.zero_state(num_samples, dtype=tf.float32)\n",
    "\n",
    "        # Unstack\n",
    "        unstacked = tf.unstack(X, axis=1)\n",
    "        # Iterate through each timestep and append the predictions\n",
    "        outputs = []\n",
    "        for input_step in unstacked:\n",
    "            output, state = self.rnn_cell(input_step, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # Stack outputs to (batch_size, time_steps, cell_size)\n",
    "        outputs = tf.stack(outputs, axis=1)\n",
    "\n",
    "        # Extract the output of the last time step, of each sample\n",
    "        idxs_last_output = tf.stack([tf.range(num_samples),\n",
    "                                     tf.cast(seq_length - 1, tf.int32)], axis=1)\n",
    "        final_output = tf.gather_nd(outputs, idxs_last_output)\n",
    "\n",
    "        x = self.dense1(final_output)\n",
    "        x = self.batch1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        x = self.batch3(x, training=training)\n",
    "        x = self.dense4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, target, seqlen, training=False):\n",
    "        predictions = self.predict(x, seqlen, training=training)\n",
    "        loss_value = tf.losses.sparse_softmax_cross_entropy(logits=predictions, labels=target)\n",
    "        return loss_value\n",
    "\n",
    "    def grads(self, x, target, seqlen,training=False):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss_value = self.loss(x, target, seqlen, training=training)\n",
    "        return tape.gradient(loss_value, self.variables)\n",
    "\n",
    "    def fit(self,\n",
    "            train_data=None,\n",
    "            eval_data=None,\n",
    "            batch_size=None,\n",
    "            epochs=500,\n",
    "            verbose=100,\n",
    "            callbacks=None,\n",
    "            validation_split=0.,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            **kwargs):\n",
    "\n",
    "        train_acc = tfe.metrics.Accuracy('train_acc')\n",
    "        eval_acc = tfe.metrics.Accuracy('eval_acc')\n",
    "\n",
    "        self.history = {}\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['eval_acc'] = []\n",
    "\n",
    "        with tf.device(self.device_name):\n",
    "            for i in range(epochs):\n",
    "                verbosity = ((i+1)%verbose == 0)\n",
    "                cnt = 0\n",
    "                for X, y, seqlen in tfe.Iterator(train_data):\n",
    "                    if cnt % 100 == 0 and verbosity:\n",
    "                        print(\"Training... %s\" % cnt)\n",
    "                    grads = self.grads(x=X, target=y, seqlen=seqlen, training=True)\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    cnt +=1\n",
    "                cnt=0\n",
    "                for X, y, seqlen in tfe.Iterator(train_data):\n",
    "                    if cnt % 100 == 0 and verbosity:\n",
    "                        print(\"logit collecting... %s\" % cnt)\n",
    "                    logits = self.predict(X, seqlen, False)\n",
    "                    preds = tf.argmax(logits, axis=1)\n",
    "                    train_acc(preds, y)\n",
    "                    cnt += 1\n",
    "                cnt=0\n",
    "                self.history['train_acc'].append(train_acc.result().numpy())\n",
    "\n",
    "                train_acc.init_variables()\n",
    "\n",
    "                # Check accuracy eval dataset\n",
    "                for X, y, seqlen in tfe.Iterator(eval_data):\n",
    "                    if cnt % 100 == 0 and verbosity:\n",
    "                        print(\"test collecting... %s\" % cnt)\n",
    "                    logits = self.predict(X, seqlen, False)\n",
    "                    preds = tf.argmax(logits, axis=1)\n",
    "                    eval_acc(preds, y)\n",
    "                    cnt +=1\n",
    "                self.history['eval_acc'].append(eval_acc.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_acc.init_variables()\n",
    "                if i==0 or verbosity:\n",
    "                    print('Train accuracy at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
    "                    print('Eval accuracy at epoch %d: ' %(i+1), self.history['eval_acc'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = DVectorNet((800,), 30, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train accuracy at epoch 1: ', 0.015970377070495787)\n",
      "('Eval accuracy at epoch 1: ', 0.022042995321603857)\n",
      "Training... 0\n",
      "logit collecting... 0\n",
      "test collecting... 0\n",
      "('Train accuracy at epoch 100: ', 0.017779410933348408)\n",
      "('Eval accuracy at epoch 100: ', 0.0257168278752045)\n",
      "Training... 0\n",
      "logit collecting... 0\n",
      "test collecting... 0\n",
      "('Train accuracy at epoch 200: ', 0.016281304765673583)\n",
      "('Eval accuracy at epoch 200: ', 0.02126804626732872)\n",
      "Training... 0\n",
      "logit collecting... 0\n",
      "test collecting... 0\n",
      "('Train accuracy at epoch 300: ', 0.037989711119905026)\n",
      "('Eval accuracy at epoch 300: ', 0.03857524181280675)\n",
      "Training... 0\n",
      "logit collecting... 0\n",
      "test collecting... 0\n",
      "('Train accuracy at epoch 400: ', 0.055910452823788793)\n",
      "('Eval accuracy at epoch 400: ', 0.060618237134410606)\n"
     ]
    }
   ],
   "source": [
    "dn.fit(ds_train, ds_test, epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
