{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "from colorama import Fore, Style\n",
    "import time\n",
    "\n",
    "# eager execution\n",
    "tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMDvector(tf.keras.Model):\n",
    "#     \"\"\"\n",
    "#     input : 98 * (dynamic length /maximum 5) * 40\n",
    "#     out : 0.66 * total speaker[depends on your dataset] =  (trained-speaker)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, input_dim, out_dim, checkpoint_directory, device_name=\"cpu:0\"):\n",
    "#         super(LSTMDvector, self).__init__()\n",
    "\n",
    "#         self.input_dim = input_dim\n",
    "#         self.out_dim = out_dim\n",
    "#         self.checkpoint_directory = checkpoint_directory\n",
    "#         self.device_name = device_name\n",
    "\n",
    "#         # dense\n",
    "#         from tensorflow.python.ops import init_ops\n",
    "        \n",
    "#         self.rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(256)\n",
    "# #         self.dense1 = tf.layers.Dense(256, activation=tf.nn.relu, kernel_initializer=init_ops.random_uniform_initializer())\n",
    "# #         self.batch1 = tf.layers.BatchNormalization()\n",
    "\n",
    "#         # dvector\n",
    "# #         self.dvector = tf.layers.Dense(128, activation=tf.nn.tanh, kernel_initializer=init_ops.random_uniform_initializer())\n",
    "\n",
    "#         self.trainprob = tf.layers.Dense(out_dim, activation=tf.nn.softmax, kernel_initializer=init_ops.random_uniform_initializer())\n",
    "\n",
    "#         self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "\n",
    "#         self.time = time.time()\n",
    "#         self.total_step = 0\n",
    "#         self.loss_sum = 0\n",
    "\n",
    "\n",
    "#     def __call__(self, X, steps=None, training=False):\n",
    "#         return self.predict(X, verbose=0, steps=None, training=False)\n",
    "\n",
    "#     def predict(self, X, seq_length, verbose=0, steps=None, training=False):\n",
    "        \n",
    "#         # Get the number of samples within a batch\n",
    "#         num_samples = tf.shape(X)[0]\n",
    "\n",
    "#         # Initialize LSTM cell state with zeros\n",
    "#         state = self.rnn_cell.zero_state(num_samples, dtype=tf.float32)\n",
    "\n",
    "#         # Unstack\n",
    "#         unstacked = tf.unstack(X, axis=1)\n",
    "#         # Iterate through each timestep and append the predictions\n",
    "#         outputs = []\n",
    "#         for input_step in unstacked:\n",
    "#             output, state = self.rnn_cell(input_step, state)\n",
    "#             outputs.append(output)\n",
    "\n",
    "#         # Stack outputs to (batch_size, time_steps, cell_size)\n",
    "#         outputs = tf.stack(outputs, axis=1)\n",
    "\n",
    "#         # Extract the output of the last time step, of each sample\n",
    "#         idxs_last_output = tf.stack([tf.range(num_samples),\n",
    "#                                      tf.cast(seq_length - 1, tf.int32)], axis=1)\n",
    "#         final_output = tf.gather_nd(outputs, idxs_last_output)\n",
    "\n",
    "        \n",
    "        \n",
    "# #         x = self.dense1(final_output)\n",
    "# #         x = self.batch1(x, training=training)\n",
    "# #         x = self.dvector(x)\n",
    "#         x = final_output\n",
    "#         x = self.trainprob(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def loss(self, x, target ,seqlen, training=False):\n",
    "#         predictions = self.predict(x, seqlen, training=training)\n",
    "#         loss_value = tf.losses.sparse_softmax_cross_entropy(logits=predictions, labels=target)\n",
    "#         self.loss_sum += loss_value\n",
    "#         return loss_value\n",
    "\n",
    "#     def grads(self, x, target,seqlen, training=False):\n",
    "#         with tfe.GradientTape() as tape:\n",
    "#             loss_value = self.loss(x, target,seqlen, training=training)\n",
    "#         return tape.gradient(loss_value, self.variables)\n",
    "\n",
    "#     def fit(self,\n",
    "#             train_data=None,\n",
    "#             eval_data=None,\n",
    "#             epochs=500,\n",
    "#             verbose=1,\n",
    "#             **kwargs):\n",
    "\n",
    "#         train_acc = tfe.metrics.Accuracy('train_acc')\n",
    "#         eval_acc = tfe.metrics.Accuracy('eval_acc')\n",
    "\n",
    "#         self.history = {}\n",
    "#         self.history['train_acc'] = []\n",
    "#         self.history['eval_acc'] = []\n",
    "\n",
    "#         with tf.device(self.device_name):\n",
    "#             for i in range(epochs):\n",
    "#                 self.total_step += 1\n",
    "#                 self.loss_sum = 0\n",
    "#                 temp = True\n",
    "\n",
    "#                 for X, y, seq in tfe.Iterator(train_data):\n",
    "#                     if i==0 and temp:\n",
    "#                         print(self.predict(X=X,seq_length=seq, training=False)[0])\n",
    "#                     grads = self.grads(x=X, target=y,seqlen=seq, training=True)\n",
    "                    \n",
    "#                     if i==0 and temp:\n",
    "#                         print(\"grads=======\")\n",
    "#                         print(grads)\n",
    "#                         print \"===========\"\n",
    "#                         temp = False\n",
    "                    \n",
    "#                     self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "#                 temp =True\n",
    "#                 if (i == 0) | ((i + 1) % verbose == 0):\n",
    "#                     for X, y, seq in tfe.Iterator(train_data):\n",
    "#                         logits = self.predict(X=X,seq_length=seq, training=False)\n",
    "#                         preds = tf.argmax(logits, axis=1)\n",
    "#                         if temp:\n",
    "#                             print(logits[0])\n",
    "#                             print(preds[0], y[0])\n",
    "#                             temp = False\n",
    "#                         train_acc(preds, y)\n",
    "\n",
    "#                     self.history['train_acc'].append(train_acc.result().numpy())\n",
    "\n",
    "#                     # Reset metrics\n",
    "#                     train_acc.init_variables()\n",
    "\n",
    "#                     # Check accuracy eval dataset\n",
    "#                     for X, y, seq in tfe.Iterator(eval_data):\n",
    "#                         logits = self.predict(X=X,seq_length=seq, training=False)\n",
    "#                         preds = tf.argmax(logits, axis=1)\n",
    "#                         eval_acc(preds, y)\n",
    "\n",
    "#                     self.history['eval_acc'].append(eval_acc.result().numpy())\n",
    "\n",
    "#                     # Reset metrics\n",
    "#                     eval_acc.init_variables()\n",
    "#                     print(Fore.CYAN + '[EPOCH %d]/%.2fsec ============================' % ((i + 1), time.time()-self.time))\n",
    "#                     self.time = time.time()\n",
    "#                     print(Fore.MAGENTA + 'Train accuracy at step %d: %5f%%' % (\n",
    "#                     self.total_step, 100.0 * self.history['train_acc'][-1]))\n",
    "#                     print(Fore.BLUE + 'Eval  accuracy at step %d: %5f%%' % (\n",
    "#                     self.total_step, 100.0 * self.history['eval_acc'][-1]) )\n",
    "#                     print(Fore.RED + 'Loss     value at step %d: %5f' % (self.total_step, self.loss_sum) + Style.RESET_ALL)\n",
    "#                     self.save(global_step=self.total_step)\n",
    "\n",
    "#     def save(self, global_step=0):\n",
    "#         tfe.Saver(self.variables).save(self.checkpoint_directory, global_step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDvector(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    input : 98 * (dynamic length /maximum 5) * 40\n",
    "    out : 0.66 * total speaker[depends on your dataset] =  (trained-speaker)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, out_dim, checkpoint_directory, device_name=\"cpu:0\"):\n",
    "        super(CNNDvector, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.device_name = device_name\n",
    "\n",
    "        # dense\n",
    "        from tensorflow.python.ops import init_ops\n",
    "        \n",
    "#         self.rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(256)\n",
    "        self.dense1 = tf.layers.Dense(256, activation=tf.nn.relu, kernel_initializer=init_ops.random_uniform_initializer())\n",
    "        self.batch1 = tf.layers.BatchNormalization()\n",
    "\n",
    "        self.dvector = tf.layers.Dense(128, activation=tf.nn.tanh, kernel_initializer=init_ops.random_uniform_initializer())\n",
    "\n",
    "        self.trainprob = tf.layers.Dense(out_dim, activation=tf.nn.softmax, kernel_initializer=init_ops.random_uniform_initializer())\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "\n",
    "        self.time = time.time()\n",
    "        self.total_step = 0\n",
    "        self.loss_sum = 0\n",
    "\n",
    "\n",
    "    def __call__(self, X, steps=None, training=False):\n",
    "        return self.predict(X, verbose=0, steps=None, training=False)\n",
    "\n",
    "    def predict(self, X, verbose=0, steps=None, training=False):    \n",
    "        \n",
    "        x = self.dense1(X)\n",
    "        x = self.batch1(x, training=training)\n",
    "        x = self.dvector(x)\n",
    "        x = self.trainprob(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, target, training=False):\n",
    "        predictions = self.predict(x, training=training)\n",
    "        loss_value = tf.losses.sparse_softmax_cross_entropy(logits=predictions, labels=target)\n",
    "        self.loss_sum += loss_value\n",
    "        return loss_value\n",
    "\n",
    "    def grads(self, x, target, training=False):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss_value = self.loss(x, target, training=training)\n",
    "        return tape.gradient(loss_value, self.variables)\n",
    "\n",
    "    def fit(self,\n",
    "            train_data=None,\n",
    "            eval_data=None,\n",
    "            epochs=500,\n",
    "            verbose=1,\n",
    "            **kwargs):\n",
    "\n",
    "        train_acc = tfe.metrics.Accuracy('train_acc')\n",
    "        eval_acc = tfe.metrics.Accuracy('eval_acc')\n",
    "\n",
    "        self.history = {}\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['eval_acc'] = []\n",
    "\n",
    "        with tf.device(self.device_name):\n",
    "            for i in range(epochs):\n",
    "                self.total_step += 1\n",
    "                self.loss_sum = 0\n",
    "                temp = True\n",
    "\n",
    "                for X, y in tfe.Iterator(train_data):\n",
    "                    if i==0 and temp:\n",
    "                        print(self.predict(X=X, training=False)[0])\n",
    "                    grads = self.grads(x=X, target=y, training=True)\n",
    "                    \n",
    "                    if i==0 and temp:\n",
    "                        print(\"grads=======\")\n",
    "                        print(grads)\n",
    "                        print \"===========\"\n",
    "                        temp = False\n",
    "                    \n",
    "                    self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                temp =True\n",
    "                if (i == 0) | ((i + 1) % verbose == 0):\n",
    "                    for X, y in tfe.Iterator(train_data):\n",
    "                        logits = self.predict(X=X,training=False)\n",
    "                        preds = tf.argmax(logits, axis=1)\n",
    "                        if temp:\n",
    "                            print(logits[0])\n",
    "                            print(preds[0], y[0])\n",
    "                            temp = False\n",
    "                        train_acc(preds, y)\n",
    "\n",
    "                    self.history['train_acc'].append(train_acc.result().numpy())\n",
    "\n",
    "                    # Reset metrics\n",
    "                    train_acc.init_variables()\n",
    "\n",
    "                    # Check accuracy eval dataset\n",
    "                    for X, y in tfe.Iterator(eval_data):\n",
    "                        logits = self.predict(X=X, training=False)\n",
    "                        preds = tf.argmax(logits, axis=1)\n",
    "                        eval_acc(preds, y)\n",
    "\n",
    "                    self.history['eval_acc'].append(eval_acc.result().numpy())\n",
    "\n",
    "                    # Reset metrics\n",
    "                    eval_acc.init_variables()\n",
    "                    print(Fore.CYAN + '[EPOCH %d]/%.2fsec ============================' % ((i + 1), time.time()-self.time))\n",
    "                    self.time = time.time()\n",
    "                    print(Fore.MAGENTA + 'Train accuracy at step %d: %5f%%' % (\n",
    "                    self.total_step, 100.0 * self.history['train_acc'][-1]))\n",
    "                    print(Fore.BLUE + 'Eval  accuracy at step %d: %5f%%' % (\n",
    "                    self.total_step, 100.0 * self.history['eval_acc'][-1]) )\n",
    "                    print(Fore.RED + 'Loss     value at step %d: %5f' % (self.total_step, self.loss_sum) + Style.RESET_ALL)\n",
    "                    self.save(global_step=self.total_step)\n",
    "\n",
    "    def save(self, global_step=0):\n",
    "        tfe.Saver(self.variables).save(self.checkpoint_directory, global_step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2805, 25, 20, 40) (2805,) (2805,)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import h5py\n",
    "\n",
    "num_data = 2\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "seq = list()\n",
    "\n",
    "for i in range(num_data):\n",
    "    fname = \"data_lmfe/data_%d.h5\" % i\n",
    "    h5f = h5py.File(fname, 'r')\n",
    "    X.append(h5f['speechs'][:].astype(np.float32))\n",
    "    y += h5f['labels'][:].tolist()\n",
    "    seq +=h5f['seqs'][:].tolist()\n",
    "    h5f.close()\n",
    "\n",
    "# Hyper parameters\n",
    "X = np.concatenate(X, axis=0)\n",
    "y = np.array(y)\n",
    "seq = np.array(seq)/20\n",
    "\n",
    "X = X.reshape(-1,25,20,40)\n",
    "\n",
    "print X.shape, y.shape, seq.shape\n",
    "print(seq[0])\n",
    "# X_5 = list()\n",
    "# for i in range(5):\n",
    "#     X_5.append(X[:, i * 100:i * 100 + 98, :])\n",
    "# X = np.array(X_5)\n",
    "# X = X.swapaxes(0, 1)\n",
    "# X = X.reshape(-1, 5, 98 * 40)\n",
    "# X = X[:,:40:2,:]\n",
    "X = X[:,1,:,:]\n",
    "X = X.reshape(-1,20*40)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 10 * num_data\n",
    "num_train = len(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90, 129, 363, 113, 102,  61,  67, 100,  72,  56, 111, 109,  79,\n",
       "       122,  61, 205, 132, 468, 303,  62])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2805, 800)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.790207 ,  -4.4402328,  -4.385018 ,  -6.627076 ,  -7.71561  ,\n",
       "        -8.209458 ,  -6.941978 ,  -4.0494823,  -2.9702692,  -4.2493734,\n",
       "        -4.640051 ,  -3.8008099,  -2.0048692,  -2.5097132,  -5.474236 ,\n",
       "        -6.2661734,  -4.8424854,  -5.25627  ,  -7.1643014,  -5.5284357,\n",
       "        -5.489832 ,  -7.7058635,  -7.407297 ,  -8.521071 ,  -5.9914627,\n",
       "        -5.869166 ,  -6.8755503,  -6.892807 ,  -7.2002263,  -7.7023907,\n",
       "        -9.35234  ,  -9.106079 ,  -9.146696 ,  -9.319186 ,  -9.534418 ,\n",
       "        -9.292682 ,  -9.008865 ,  -8.614171 ,  -6.5368466,  -6.0457773,\n",
       "        -6.4704933,  -4.2238827,  -4.2684774,  -6.797896 ,  -7.4545913,\n",
       "        -7.902767 ,  -7.3521295,  -3.7103693,  -3.117904 ,  -4.2578125,\n",
       "        -5.1057234,  -4.948519 ,  -3.701523 ,  -3.6010492,  -5.4572625,\n",
       "        -6.598808 ,  -5.504063 ,  -5.9570527,  -7.866792 ,  -6.67752  ,\n",
       "        -6.3727407,  -8.265967 ,  -7.561988 ,  -8.333328 ,  -7.3799257,\n",
       "        -6.9370317,  -7.9638114,  -7.6044507,  -8.967032 ,  -8.599475 ,\n",
       "        -9.856191 , -10.023173 ,  -9.812897 , -10.677408 , -10.637784 ,\n",
       "       -10.459256 , -10.423381 ,  -9.501163 ,  -7.137073 ,  -7.1457095,\n",
       "        -6.4881086,  -3.9514232,  -3.2283862,  -4.229967 ,  -5.462501 ,\n",
       "        -6.896014 ,  -5.6969748,  -4.232221 ,  -3.8132086,  -4.940804 ,\n",
       "        -4.938546 ,  -5.746289 ,  -5.9592   ,  -6.079056 ,  -7.1746507,\n",
       "        -8.083585 ,  -7.885453 ,  -7.9294744,  -8.945162 ,  -8.188071 ,\n",
       "        -8.274338 ,  -9.356944 ,  -9.321939 ,  -9.245191 ,  -8.931691 ,\n",
       "        -7.9206963,  -8.2043085,  -8.808303 ,  -9.500107 ,  -9.504105 ,\n",
       "       -10.345717 , -10.063629 , -10.190122 , -10.859936 , -10.817953 ,\n",
       "       -10.963596 , -10.793249 , -10.482818 ,  -9.226063 ,  -8.889264 ,\n",
       "        -6.0302486,  -3.735952 ,  -2.693239 ,  -3.621103 ,  -6.3008604,\n",
       "        -6.4292803,  -7.152722 ,  -6.5176787,  -4.4355264,  -4.080756 ,\n",
       "        -6.0069013,  -7.1012506,  -8.309561 ,  -7.05932  ,  -6.8469167,\n",
       "        -8.1014185,  -8.28934  ,  -9.222394 ,  -9.000732 ,  -8.312228 ,\n",
       "        -8.495325 ,  -9.457932 ,  -9.703417 ,  -9.745126 ,  -9.028465 ,\n",
       "        -8.526683 ,  -9.810058 ,  -9.903563 , -10.46955  , -10.30809  ,\n",
       "       -10.430274 , -10.694601 , -10.51542  , -10.70974  , -10.7046585,\n",
       "       -10.938757 , -10.750764 , -10.321348 , -10.263561 , -10.920652 ,\n",
       "        -5.599933 ,  -3.7281048,  -2.3196611,  -3.0507917,  -5.6577377,\n",
       "        -6.085892 ,  -6.4045496,  -6.633571 ,  -4.5217323,  -3.640469 ,\n",
       "        -4.9898396,  -6.560093 ,  -7.1594243,  -5.924419 ,  -5.963125 ,\n",
       "        -8.678112 ,  -8.780646 ,  -9.205709 ,  -9.10631  ,  -8.954076 ,\n",
       "        -9.478698 ,  -9.714603 , -10.075865 ,  -9.540628 ,  -8.983414 ,\n",
       "        -8.882968 ,  -9.301472 , -10.044831 ,  -9.750633 , -10.003874 ,\n",
       "       -10.321926 , -10.287558 , -10.106478 , -10.306035 , -10.690943 ,\n",
       "       -10.930633 , -10.68249  , -10.3441   , -10.073461 , -10.184064 ,\n",
       "        -5.291708 ,  -3.9658136,  -2.106724 ,  -2.5194383,  -5.1911125,\n",
       "        -5.822269 ,  -5.8574767,  -5.5855956,  -4.6713357,  -3.4910989,\n",
       "        -4.9254193,  -7.782571 ,  -8.336219 ,  -6.5316   ,  -5.6084714,\n",
       "        -8.399231 ,  -8.941769 ,  -9.881116 , -11.2767935, -10.339216 ,\n",
       "        -9.924222 , -10.696023 , -11.562143 , -10.138356 ,  -9.730772 ,\n",
       "        -9.5052805,  -8.740301 , -10.837333 , -11.129733 , -11.270367 ,\n",
       "       -11.169454 , -11.476922 , -10.850216 , -11.487028 , -11.934949 ,\n",
       "       -12.085112 , -12.081261 , -10.905303 ,  -9.983461 , -10.008618 ,\n",
       "        -4.89901  ,  -3.7842393,  -1.9976382,  -2.5333343,  -5.3298965,\n",
       "        -5.7721276,  -8.07069  ,  -7.045034 ,  -5.4079304,  -3.4377303,\n",
       "        -4.3362513,  -6.3774605,  -6.8290873,  -5.652331 ,  -5.3563395,\n",
       "        -8.994416 ,  -9.230587 ,  -9.287995 ,  -9.281988 ,  -9.480712 ,\n",
       "        -9.504161 ,  -9.830779 ,  -9.714747 ,  -9.564934 ,  -9.573864 ,\n",
       "        -9.192398 ,  -8.4865055,  -9.863767 , -10.031296 , -10.005358 ,\n",
       "        -9.965558 , -10.414109 , -10.819723 , -10.661195 , -10.502698 ,\n",
       "       -10.671967 , -10.932232 , -10.711477 ,  -9.677837 ,  -9.93182  ,\n",
       "        -5.5047517,  -4.235757 ,  -2.1061916,  -2.2434733,  -4.669029 ,\n",
       "        -5.1306925,  -5.9022894,  -5.7249675,  -4.9017367,  -3.3733459,\n",
       "        -4.4623466,  -7.2771945,  -7.2365303,  -6.2186246,  -5.246076 ,\n",
       "        -7.8152165, -10.239441 , -11.105806 , -11.867328 , -10.755001 ,\n",
       "       -11.050385 , -11.736241 , -11.285586 , -10.40555  ,  -9.576582 ,\n",
       "        -9.341321 ,  -8.87501  ,  -9.745855 , -10.654297 , -10.711881 ,\n",
       "       -10.446504 , -10.835505 ,  -9.942034 , -11.36952  , -11.505927 ,\n",
       "       -11.760537 , -11.692913 , -11.229881 , -10.197339 , -10.449865 ,\n",
       "        -5.015109 ,  -4.2992325,  -2.1609938,  -2.326715 ,  -4.8529882,\n",
       "        -5.9008455,  -7.233976 ,  -7.015076 ,  -6.445714 ,  -3.6669135,\n",
       "        -3.924066 ,  -6.0082574,  -6.6891356,  -5.8532267,  -5.151465 ,\n",
       "        -7.6821346,  -8.285289 ,  -8.573404 ,  -8.898715 ,  -8.91226  ,\n",
       "        -9.210852 ,  -9.194305 ,  -9.52977  ,  -9.148797 ,  -8.893803 ,\n",
       "        -9.04598  ,  -9.272984 ,  -9.480294 ,  -9.7468605,  -9.811534 ,\n",
       "        -9.846471 , -10.733741 ,  -9.680114 , -10.342336 , -10.226863 ,\n",
       "       -10.299584 , -10.639408 , -10.349091 , -10.00968  ,  -9.717165 ,\n",
       "        -5.7532053,  -4.767031 ,  -2.4180322,  -2.3336692,  -4.5018916,\n",
       "        -5.259783 ,  -5.99118  ,  -5.79528  ,  -5.3211074,  -3.3666768,\n",
       "        -4.1243296,  -7.6582103,  -7.8597636,  -7.055233 ,  -5.5172753,\n",
       "        -7.518611 ,  -9.702636 , -10.967408 , -11.970471 , -11.293405 ,\n",
       "       -11.113581 , -10.46233  , -11.178403 ,  -9.85126  ,  -8.9226885,\n",
       "        -9.878693 , -10.16049  , -10.891891 , -10.403635 , -10.556211 ,\n",
       "       -10.758355 , -11.559981 , -10.33052  , -11.961375 , -11.77638  ,\n",
       "       -11.306057 , -11.309223 , -11.020728 , -10.611899 , -10.535551 ,\n",
       "        -5.4141116,  -4.843041 ,  -2.5830977,  -2.554819 ,  -4.796462 ,\n",
       "        -5.301945 ,  -6.13822  ,  -6.2504616,  -6.3898067,  -3.820019 ,\n",
       "        -3.382682 ,  -4.741762 ,  -5.1708674,  -5.3179636,  -4.729655 ,\n",
       "        -5.231413 ,  -5.936902 ,  -6.358559 ,  -7.1359386,  -8.567211 ,\n",
       "        -9.087578 ,  -7.531178 ,  -7.549447 ,  -7.4842515,  -6.9146876,\n",
       "        -7.9156847,  -8.700566 ,  -9.449011 , -10.26621  ,  -9.897844 ,\n",
       "        -9.528411 ,  -9.336404 ,  -9.515708 , -10.600563 , -10.168495 ,\n",
       "       -10.43628  ,  -9.508984 ,  -8.566948 ,  -8.063039 ,  -7.3960085,\n",
       "        -5.9500675,  -5.5699563,  -3.167997 ,  -2.7660248,  -4.266436 ,\n",
       "        -6.826979 ,  -6.7536764,  -6.154063 ,  -4.571133 ,  -4.239925 ,\n",
       "        -2.343746 ,  -3.0886805,  -4.674198 ,  -4.8335533,  -3.3341668,\n",
       "        -2.8604195,  -4.4018774,  -4.904455 ,  -4.0741878,  -5.3713355,\n",
       "        -7.2278957,  -5.5239263,  -5.921437 ,  -6.8324385,  -5.0966067,\n",
       "        -5.895935 ,  -5.2267923,  -5.9354215,  -7.8808503,  -7.661739 ,\n",
       "        -8.774482 ,  -8.78951  ,  -9.6710005,  -9.892878 , -10.623682 ,\n",
       "        -9.577802 ,  -8.095089 ,  -6.691997 ,  -5.2865043,  -3.4675117,\n",
       "        -6.9284973,  -6.4074364,  -4.3015847,  -3.546559 ,  -4.7865577,\n",
       "        -5.7667723,  -6.1185546,  -5.7203155,  -5.138337 ,  -3.3052938,\n",
       "        -2.6046941,  -4.066187 ,  -5.0602703,  -5.0820403,  -2.5903761,\n",
       "        -2.232079 ,  -4.311147 ,  -5.0532184,  -2.243843 ,  -2.4717674,\n",
       "        -4.5858397,  -4.542635 ,  -4.5427246,  -6.415518 ,  -4.141459 ,\n",
       "        -4.5435658,  -4.9198847,  -4.3774385,  -7.369443 ,  -6.548351 ,\n",
       "        -8.051621 ,  -7.7509193,  -9.686331 ,  -9.8656645, -10.0111   ,\n",
       "        -9.180133 ,  -7.4962287,  -5.316297 ,  -3.813547 ,  -2.7711737,\n",
       "        -7.471854 ,  -6.9160566,  -4.9162583,  -4.143327 ,  -5.5320125,\n",
       "        -6.8940897,  -7.3811226,  -7.0479536,  -6.4373775,  -4.80926  ,\n",
       "        -3.2165034,  -5.1398783,  -6.355316 ,  -5.442402 ,  -3.1692083,\n",
       "        -2.441466 ,  -5.456284 ,  -4.6582623,  -2.0847037,  -1.0371835,\n",
       "        -3.322497 ,  -4.863649 ,  -3.7643855,  -5.6330104,  -4.1158357,\n",
       "        -3.7294655,  -5.44626  ,  -4.286428 ,  -6.524377 ,  -6.663097 ,\n",
       "        -8.02283  ,  -7.771189 ,  -8.937088 ,  -9.847053 ,  -9.698003 ,\n",
       "        -8.5329   ,  -5.901313 ,  -4.691114 ,  -3.612887 ,  -2.8725436,\n",
       "        -7.62246  ,  -7.131228 ,  -5.381431 ,  -4.1526885,  -5.152863 ,\n",
       "        -7.5468535,  -7.648905 ,  -7.4543877,  -7.1934457,  -5.8767176,\n",
       "        -3.5960014,  -4.876777 ,  -6.729296 ,  -6.337002 ,  -4.521995 ,\n",
       "        -3.0164053,  -4.5916266,  -4.787295 ,  -2.728387 ,  -0.6988746,\n",
       "        -2.4200037,  -4.864773 ,  -4.566801 ,  -5.876089 ,  -5.333626 ,\n",
       "        -3.708937 ,  -5.5373044,  -4.8559127,  -5.633622 ,  -7.1863704,\n",
       "        -7.1962934,  -7.4914107,  -7.798166 ,  -8.683494 ,  -8.724519 ,\n",
       "        -8.536811 ,  -5.3548326,  -4.814326 ,  -3.9851604,  -3.068868 ,\n",
       "        -8.860174 ,  -8.126406 ,  -5.8816767,  -4.1501374,  -4.5142493,\n",
       "        -6.9493785,  -6.858281 ,  -6.973826 ,  -6.9224763,  -6.1097627,\n",
       "        -3.9275942,  -4.800126 ,  -6.753251 ,  -6.5123415,  -5.1348767,\n",
       "        -3.3272114,  -4.5749307,  -5.7061825,  -3.7552671,  -1.047832 ,\n",
       "        -2.1120408,  -5.16304  ,  -5.500042 ,  -5.65895  ,  -5.745226 ,\n",
       "        -4.151214 ,  -5.6202335,  -5.5318403,  -5.1940503,  -7.8634095,\n",
       "        -7.616125 ,  -8.773486 ,  -7.845268 , -10.017157 , -10.223785 ,\n",
       "        -8.602856 ,  -5.728014 ,  -5.114026 ,  -4.154871 ,  -3.2723765,\n",
       "        -7.724434 ,  -6.699372 ,  -5.7881474,  -3.9811256,  -4.424081 ,\n",
       "        -6.9878693,  -7.6573887,  -7.651824 ,  -7.6898756,  -6.771187 ,\n",
       "        -3.7494519,  -4.353801 ,  -6.598847 ,  -6.7026534,  -5.726028 ,\n",
       "        -3.276218 ,  -4.046201 ,  -5.755139 ,  -4.4830627,  -1.9741358,\n",
       "        -2.6586437,  -5.253188 ,  -5.3455806,  -5.389261 ,  -6.731044 ,\n",
       "        -5.2039666,  -5.206886 ,  -5.376789 ,  -5.070909 ,  -7.301909 ,\n",
       "        -7.3106985,  -8.730789 ,  -8.525012 ,  -8.710655 ,  -8.732148 ,\n",
       "        -8.012063 ,  -6.0924625,  -5.2674184,  -3.8118346,  -3.5118618,\n",
       "        -8.767063 ,  -7.213259 ,  -6.3367596,  -3.9009979,  -3.901122 ,\n",
       "        -6.1881213,  -6.514049 ,  -6.769928 ,  -6.853682 ,  -6.261677 ,\n",
       "        -3.5616891,  -3.88406  ,  -6.70146  ,  -6.729964 ,  -5.78891  ,\n",
       "        -3.071426 ,  -3.6380193,  -6.272721 ,  -5.838302 ,  -3.2727277,\n",
       "        -3.513955 ,  -6.046073 ,  -5.8788695,  -5.584788 ,  -7.924842 ,\n",
       "        -6.9545393,  -6.97893  ,  -6.25369  ,  -4.4547853,  -6.0586   ,\n",
       "        -7.0519657,  -7.8295016,  -7.515267 ,  -8.876454 ,  -9.25672  ,\n",
       "        -8.811407 ,  -6.979109 ,  -5.609108 ,  -3.943919 ,  -3.8782418,\n",
       "        -8.177033 ,  -7.0548725,  -5.933976 ,  -3.6367426,  -3.7383394,\n",
       "        -6.2037964,  -6.6796703,  -6.9679255,  -7.063004 ,  -6.301284 ,\n",
       "        -3.4825273,  -3.5752912,  -6.636936 ,  -7.0184574,  -5.7913504,\n",
       "        -2.7288535,  -2.9955516,  -5.7368784,  -6.46131  ,  -4.1468835,\n",
       "        -3.8411527,  -6.2380013,  -6.196665 ,  -5.404404 ,  -6.8041716,\n",
       "        -6.469571 ,  -6.1784616,  -6.7673326,  -3.3995328,  -4.6062107,\n",
       "        -6.284179 ,  -6.5821166,  -6.1307316,  -6.5679774,  -6.787487 ,\n",
       "        -7.0418468,  -6.729152 ,  -6.052125 ,  -4.569174 ,  -4.2572246,\n",
       "        -7.686258 ,  -6.677597 ,  -6.0242257,  -3.7176971,  -3.6959856,\n",
       "        -5.9379864,  -6.710106 ,  -7.0660763,  -7.426802 ,  -6.2791452,\n",
       "        -3.3870933,  -3.1788862,  -5.6315923,  -5.9264565,  -5.0713725,\n",
       "        -2.3098173,  -2.514706 ,  -5.247985 ,  -5.5528445,  -4.3224335,\n",
       "        -4.1373925,  -7.0194626,  -6.9751887,  -6.245192 ,  -7.7572684,\n",
       "        -6.470712 ,  -7.015571 ,  -6.9398985,  -3.367819 ,  -4.301156 ,\n",
       "        -4.748098 ,  -5.3078012,  -5.666492 ,  -6.4135766,  -7.0967536,\n",
       "        -8.017546 ,  -8.057129 ,  -6.83268  ,  -4.882438 ,  -4.0626755],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, seq_train, seq_test = train_test_split(X, y,seq, test_size=0.33, random_state=42)\n",
    "# del X, y\n",
    "# import gc\n",
    "# gc.collect()\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train )).batch(1)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1)\n",
    "# model = CNNDvector((98 * 40,), num_classes, \"checkpoints/\", device_name=\"gpu:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05\n",
      " 0.05 0.05 0.05 0.05 0.05 0.05], shape=(20,), dtype=float32)\n",
      "grads=======\n",
      "[<tf.Tensor: id=630, shape=(20000, 256), dtype=float32, numpy=\n",
      "array([[-0., -0., -0., ..., -0., -0., -0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       ...,\n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.],\n",
      "       [-0., -0., -0., ..., -0., -0., -0.]], dtype=float32)>, <tf.Tensor: id=628, shape=(256,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Tensor: id=570, shape=(256,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>, <tf.Tensor: id=552, shape=(256,), dtype=float32, numpy=\n",
      "array([-0.03378471,  0.03233083,  0.07624447,  0.00110361,  0.07542127,\n",
      "        0.0741018 ,  0.10035333,  0.04173988,  0.01546106,  0.01014158,\n",
      "        0.12461793, -0.01396574,  0.00992792, -0.05016709,  0.04308118,\n",
      "        0.04606378,  0.07686181,  0.06896129,  0.01439764, -0.00397906,\n",
      "        0.02696851, -0.00898907,  0.04136152,  0.00776893,  0.02115702,\n",
      "       -0.05953983,  0.02022132, -0.03698808,  0.07596673,  0.07286559,\n",
      "       -0.02709071, -0.01599624, -0.04774373,  0.03630334, -0.00299186,\n",
      "       -0.04353267,  0.02780552,  0.03317045,  0.0731293 ,  0.02452001,\n",
      "       -0.0063026 , -0.00640783,  0.05265745, -0.02388149,  0.04871709,\n",
      "        0.00872704,  0.04933351, -0.00777367,  0.02275487,  0.05735238,\n",
      "        0.08038747, -0.0131739 , -0.02324228, -0.00869353,  0.08557965,\n",
      "        0.07271941, -0.04809442,  0.0088083 ,  0.08032007,  0.03731424,\n",
      "        0.13481818,  0.02880364, -0.01429384,  0.01222875,  0.02164984,\n",
      "        0.07041516, -0.04194206, -0.05338191, -0.03731032, -0.00020326,\n",
      "        0.04862399,  0.03801227,  0.02611331,  0.06671239,  0.11751591,\n",
      "       -0.01994783, -0.02121549,  0.08804483, -0.04219648, -0.07260905,\n",
      "        0.00273474,  0.03692934,  0.0470392 ,  0.08981805,  0.00361299,\n",
      "       -0.06359065,  0.04343591,  0.01726826,  0.02919938, -0.03108494,\n",
      "       -0.03592312,  0.00960121,  0.09331842, -0.05215812, -0.03550764,\n",
      "        0.09591249,  0.00933028, -0.08072441, -0.01499455,  0.0134643 ,\n",
      "       -0.04548428,  0.03276685,  0.04286724,  0.02191182, -0.05523978,\n",
      "       -0.06402066,  0.09174071,  0.02603145,  0.05767986,  0.05252296,\n",
      "       -0.01268478,  0.06048061,  0.00320166,  0.03641276,  0.09429336,\n",
      "        0.00380582,  0.0670048 ,  0.13192604, -0.05078965,  0.04596327,\n",
      "       -0.00251429, -0.02205425, -0.03183151,  0.04529315, -0.02524928,\n",
      "        0.02517558,  0.08840707,  0.06909836, -0.01258187, -0.043432  ,\n",
      "        0.06758867,  0.01344036,  0.03814292,  0.11367913, -0.03226125,\n",
      "        0.08670251,  0.02347605,  0.02682484,  0.10075651,  0.01869144,\n",
      "        0.00831336,  0.00528838,  0.06592168,  0.02398615,  0.00691782,\n",
      "       -0.02088777,  0.01851099, -0.08865542,  0.06791142,  0.03656658,\n",
      "        0.04952331, -0.02944598,  0.05554904,  0.01886256,  0.02136985,\n",
      "        0.02923597, -0.06450211,  0.08639908,  0.07345585,  0.03732372,\n",
      "        0.02289556,  0.09562968,  0.06421541,  0.04379426,  0.01309093,\n",
      "       -0.01297224,  0.0459407 , -0.01312772,  0.00712374,  0.0075858 ,\n",
      "        0.01384508,  0.0897111 ,  0.0694756 ,  0.08716504,  0.04566285,\n",
      "        0.07305337,  0.0500901 ,  0.05805476,  0.06332576,  0.06081291,\n",
      "       -0.01951148,  0.05362388,  0.00043174,  0.11073331,  0.11008702,\n",
      "       -0.03360871, -0.03003879,  0.03188979,  0.08244445, -0.00976084,\n",
      "        0.09872647,  0.08088771,  0.04903818,  0.07622153,  0.00385009,\n",
      "       -0.03138811, -0.02157439,  0.07136571,  0.01092607,  0.05518097,\n",
      "        0.08650085,  0.02445576,  0.05645659, -0.01255654,  0.09108391,\n",
      "       -0.00372295,  0.02333161,  0.03646568, -0.00864439,  0.08654125,\n",
      "       -0.01370399,  0.04699416,  0.02325472,  0.04830633,  0.03606147,\n",
      "        0.01562425,  0.02903368,  0.00909051,  0.04391121,  0.05860943,\n",
      "        0.13260284,  0.02032062, -0.05082396, -0.0059419 ,  0.02944078,\n",
      "        0.01414151,  0.05338551,  0.02414118,  0.03441824,  0.06357665,\n",
      "        0.07182837,  0.03541847,  0.07746048,  0.06235347,  0.03054575,\n",
      "       -0.00598405,  0.04322556,  0.02886011,  0.14495723,  0.01501707,\n",
      "        0.01895682,  0.01332069, -0.01543911,  0.04225109,  0.06234808,\n",
      "        0.04553215,  0.02642839,  0.05797022,  0.07581204,  0.02384218,\n",
      "       -0.06535186,  0.00479772,  0.0670871 ,  0.00948982, -0.01679395,\n",
      "        0.06162662], dtype=float32)>, <tf.Tensor: id=544, shape=(256, 128), dtype=float32, numpy=\n",
      "array([[ 0.,  0.,  0., ..., -0.,  0., -0.],\n",
      "       [ 0.,  0.,  0., ..., -0.,  0., -0.],\n",
      "       [ 0.,  0.,  0., ..., -0.,  0., -0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ..., -0.,  0., -0.],\n",
      "       [ 0.,  0.,  0., ..., -0.,  0., -0.],\n",
      "       [ 0.,  0.,  0., ..., -0.,  0., -0.]], dtype=float32)>, <tf.Tensor: id=542, shape=(128,), dtype=float32, numpy=\n",
      "array([ 2.02826448e-02,  1.21347280e-02,  8.20414070e-03,  1.68166794e-02,\n",
      "       -2.43041087e-02, -1.19785992e-02,  1.45582920e-02, -1.73330735e-02,\n",
      "       -1.06302993e-02,  1.15457028e-02,  8.93580168e-03,  1.30015230e-02,\n",
      "        2.85889935e-02,  9.24080051e-03, -1.28842508e-02,  9.79344640e-03,\n",
      "        1.30541855e-02,  2.16215905e-02,  1.16240010e-02, -4.20424808e-03,\n",
      "        1.75889172e-02,  7.09836837e-03,  2.40388094e-03,  9.94544011e-03,\n",
      "        2.74869753e-03, -2.14600302e-02, -6.31572818e-03, -2.53740232e-04,\n",
      "       -2.32946761e-02, -2.70810770e-03,  8.05434398e-03, -1.26610221e-02,\n",
      "       -2.13396773e-02,  9.26892832e-03,  1.77482422e-02,  2.09026900e-03,\n",
      "       -1.64702795e-02, -1.58081874e-02, -2.18832735e-02,  8.55820160e-03,\n",
      "        1.64381433e-02, -1.74054950e-02, -1.01577975e-02, -2.51588807e-03,\n",
      "        9.75175109e-03,  1.59815699e-02, -2.39821561e-02,  2.48391274e-02,\n",
      "        1.80791020e-02,  2.61113560e-03,  4.86391364e-05,  1.34152938e-02,\n",
      "        2.14546807e-02,  1.12066772e-02, -2.45084297e-02,  2.34198850e-02,\n",
      "        2.56913062e-03, -1.48835303e-02, -1.58173405e-02,  8.05383828e-03,\n",
      "        1.83373205e-02,  1.78075046e-03, -2.05399040e-02, -3.00271367e-03,\n",
      "       -1.01562506e-02, -2.45938972e-02, -1.98012441e-02,  1.04614533e-02,\n",
      "       -1.91449430e-02, -1.50668658e-02, -4.69299778e-03,  9.53064114e-03,\n",
      "       -1.53072746e-02,  2.22926736e-02, -1.54329622e-02, -1.78416204e-02,\n",
      "        1.88519079e-02,  1.94469728e-02, -1.22329360e-02, -5.40071912e-03,\n",
      "        1.09207602e-02, -3.12055508e-03,  2.35465504e-02,  1.91573184e-02,\n",
      "        5.05575445e-03, -3.74894030e-03,  2.72386484e-02, -9.66298487e-03,\n",
      "       -5.03038429e-03,  2.32756045e-03, -1.44642508e-02,  1.32986987e-02,\n",
      "       -8.00351030e-04,  1.47458650e-02, -1.98666342e-02, -2.88223801e-03,\n",
      "       -9.31493752e-03, -2.08053514e-02,  1.67738576e-03,  6.68732915e-03,\n",
      "        6.11834228e-03, -1.76251195e-02,  3.65161849e-03, -1.02968281e-02,\n",
      "        2.27287281e-02,  4.69124131e-03,  1.45946844e-02, -5.42223360e-03,\n",
      "       -2.28693914e-02, -2.70333104e-02, -1.95053034e-02, -4.16687835e-04,\n",
      "        4.52860165e-03,  2.01357398e-02,  1.85033847e-02,  8.13111314e-04,\n",
      "        1.29896542e-02,  2.78127403e-03,  2.97601614e-03, -4.33143228e-03,\n",
      "       -2.21847603e-03,  6.43846253e-03, -7.64504774e-04, -8.14803783e-03,\n",
      "       -1.06780473e-02, -2.35864632e-02,  1.48498900e-02, -3.18586593e-04],\n",
      "      dtype=float32)>, <tf.Tensor: id=540, shape=(128, 20), dtype=float32, numpy=\n",
      "array([[ 0., -0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0., -0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0., -0.,  0., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0., -0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0., -0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0., -0.,  0., ...,  0.,  0.,  0.]], dtype=float32)>, <tf.Tensor: id=538, shape=(20,), dtype=float32, numpy=\n",
      "array([ 0.0025, -0.0475,  0.0025,  0.0025,  0.0025,  0.0025,  0.0025,\n",
      "        0.0025,  0.0025,  0.0025,  0.0025,  0.0025,  0.0025,  0.0025,\n",
      "        0.0025,  0.0025,  0.0025,  0.0025,  0.0025,  0.0025],\n",
      "      dtype=float32)>, None, None]\n",
      "===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[3.1658259e-12 1.0000000e+00 5.1221614e-13 2.1746586e-13 3.3307803e-12\n",
      " 3.3551739e-13 8.0546946e-13 2.1477086e-13 1.1954899e-12 1.1428705e-11\n",
      " 3.7320884e-13 2.8846717e-13 2.8307876e-13 2.0026188e-12 1.8534412e-12\n",
      " 6.6147737e-14 1.2284520e-12 2.9815817e-13 7.0909646e-13 4.7393030e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=26931, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=26935, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 1]/2.33sec ============================\n",
      "\u001b[35mTrain accuracy at step 1: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 1: 51.666667%\n",
      "\u001b[31mLoss     value at step 1: 312.702820\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1649444e-12 1.0000000e+00 5.1207351e-13 2.1740201e-13 3.3299674e-12\n",
      " 3.3542781e-13 8.0524823e-13 2.1470944e-13 1.1951661e-12 1.1425959e-11\n",
      " 3.7310069e-13 2.8839455e-13 2.8299563e-13 2.0020689e-12 1.8529605e-12\n",
      " 6.6126798e-14 1.2281146e-12 2.9807287e-13 7.0890982e-13 4.7379657e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=138685, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=138689, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 5]/5.22sec ============================\n",
      "\u001b[35mTrain accuracy at step 5: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 5: 51.666667%\n",
      "\u001b[31mLoss     value at step 5: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.16494444e-12 1.00000000e+00 5.12071559e-13 2.17402011e-13\n",
      " 3.32995458e-12 3.35426538e-13 8.05248229e-13 2.14709435e-13\n",
      " 1.19516614e-12 1.14256105e-11 3.73100693e-13 2.88394552e-13\n",
      " 2.82994548e-13 2.00206106e-12 1.85296049e-12 6.61267983e-14\n",
      " 1.22810995e-12 2.98072873e-13 7.08909819e-13 4.73794777e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=276698, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=276702, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 10]/6.07sec ============================\n",
      "\u001b[35mTrain accuracy at step 10: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 10: 51.666667%\n",
      "\u001b[31mLoss     value at step 10: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1649444e-12 1.0000000e+00 5.1207351e-13 2.1740034e-13 3.3299546e-12\n",
      " 3.3542781e-13 8.0525132e-13 2.1470859e-13 1.1951661e-12 1.1424869e-11\n",
      " 3.7310069e-13 2.8839455e-13 2.8299455e-13 2.0020611e-12 1.8529536e-12\n",
      " 6.6127049e-14 1.2281100e-12 2.9807287e-13 7.0890982e-13 4.7379657e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=414711, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=414715, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 15]/5.92sec ============================\n",
      "\u001b[35mTrain accuracy at step 15: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 15: 51.666667%\n",
      "\u001b[31mLoss     value at step 15: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1649444e-12 1.0000000e+00 5.1207546e-13 2.1740201e-13 3.3299674e-12\n",
      " 3.3542909e-13 8.0525132e-13 2.1470944e-13 1.1951707e-12 1.1423606e-11\n",
      " 3.7310210e-13 2.8839236e-13 2.8299455e-13 2.0020689e-12 1.8529536e-12\n",
      " 6.6127049e-14 1.2281194e-12 2.9807287e-13 7.0890982e-13 4.7379657e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=552724, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=552728, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 20]/6.16sec ============================\n",
      "\u001b[35mTrain accuracy at step 20: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 20: 51.666667%\n",
      "\u001b[31mLoss     value at step 20: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1649564e-12 1.0000000e+00 5.1207156e-13 2.1740201e-13 3.3299925e-12\n",
      " 3.3543036e-13 8.0525132e-13 2.1470859e-13 1.1951798e-12 1.1421296e-11\n",
      " 3.7310210e-13 2.8839347e-13 2.8299455e-13 2.0020689e-12 1.8529605e-12\n",
      " 6.6126297e-14 1.2281194e-12 2.9807287e-13 7.0891248e-13 4.7379478e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=690737, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=690741, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 25]/5.94sec ============================\n",
      "\u001b[35mTrain accuracy at step 25: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 25: 51.666667%\n",
      "\u001b[31mLoss     value at step 25: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1650229e-12 1.0000000e+00 5.1207644e-13 2.1740159e-13 3.3300246e-12\n",
      " 3.3543101e-13 8.0525289e-13 2.1470984e-13 1.1951912e-12 1.1416743e-11\n",
      " 3.7310850e-13 2.8839729e-13 2.8299509e-13 2.0020728e-12 1.8529924e-12\n",
      " 6.6127679e-14 1.2281264e-12 2.9807686e-13 7.0892738e-13 4.7379928e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=828750, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=828754, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 30]/6.37sec ============================\n",
      "\u001b[35mTrain accuracy at step 30: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 30: 51.666667%\n",
      "\u001b[31mLoss     value at step 30: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1651736e-12 1.0000000e+00 5.1208717e-13 2.1740864e-13 3.3301198e-12\n",
      " 3.3544573e-13 8.0527284e-13 2.1471598e-13 1.1952208e-12 1.1410191e-11\n",
      " 3.7313197e-13 2.8840886e-13 2.8300642e-13 2.0021452e-12 1.8530737e-12\n",
      " 6.6130579e-14 1.2281709e-12 2.9808881e-13 7.0895850e-13 4.7382373e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=966763, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=966767, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 35]/5.99sec ============================\n",
      "\u001b[35mTrain accuracy at step 35: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 35: 51.666667%\n",
      "\u001b[31mLoss     value at step 35: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.16557197e-12 1.00000000e+00 5.12114494e-13 2.17424399e-13\n",
      " 3.33034837e-12 3.35473890e-13 8.05325045e-13 2.14728246e-13\n",
      " 1.19533029e-12 1.14016635e-11 3.73177536e-13 2.88428651e-13\n",
      " 2.83029107e-13 2.00229026e-12 1.85325041e-12 6.61378978e-14\n",
      " 1.22826001e-12 2.98119494e-13 7.09036942e-13 4.73859829e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1104776, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1104780, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 40]/5.88sec ============================\n",
      "\u001b[35mTrain accuracy at step 40: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 40: 51.666667%\n",
      "\u001b[31mLoss     value at step 40: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1667678e-12 1.0000000e+00 5.1222980e-13 2.1747996e-13 3.3310726e-12\n",
      " 3.3556090e-13 8.0550323e-13 2.1477577e-13 1.1956038e-12 1.1390882e-11\n",
      " 3.7330994e-13 2.8851230e-13 2.8310253e-13 2.0027868e-12 1.8537665e-12\n",
      " 6.6161120e-14 1.2285270e-12 2.9820824e-13 7.0926956e-13 4.7398999e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1242789, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1242793, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 45]/5.91sec ============================\n",
      "\u001b[35mTrain accuracy at step 45: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 45: 51.666667%\n",
      "\u001b[31mLoss     value at step 45: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1694389e-12 1.0000000e+00 5.1249955e-13 2.1761774e-13 3.3327375e-12\n",
      " 3.3578115e-13 8.0593355e-13 2.1489460e-13 1.1962609e-12 1.1378071e-11\n",
      " 3.7359344e-13 2.8869943e-13 2.8328183e-13 2.0040248e-12 1.8549053e-12\n",
      " 6.6213880e-14 1.2291739e-12 2.9842331e-13 7.0979469e-13 4.7427578e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1380802, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1380806, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 50]/5.98sec ============================\n",
      "\u001b[35mTrain accuracy at step 50: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 50: 51.666667%\n",
      "\u001b[31mLoss     value at step 50: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1738668e-12 1.0000000e+00 5.1295719e-13 2.1785445e-13 3.3355100e-12\n",
      " 3.3613742e-13 8.0665330e-13 2.1509143e-13 1.1973246e-12 1.1361676e-11\n",
      " 3.7405978e-13 2.8900466e-13 2.8358674e-13 2.0060748e-12 1.8567958e-12\n",
      " 6.6303103e-14 1.2302341e-12 2.9877302e-13 7.1065349e-13 4.7474822e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1518815, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1518819, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 55]/5.96sec ============================\n",
      "\u001b[35mTrain accuracy at step 55: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 55: 51.666667%\n",
      "\u001b[31mLoss     value at step 55: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1811517e-12 1.0000000e+00 5.1371499e-13 2.1824290e-13 3.3401324e-12\n",
      " 3.3671622e-13 8.0784191e-13 2.1541413e-13 1.1990157e-12 1.1341843e-11\n",
      " 3.7481680e-13 2.8950231e-13 2.8408591e-13 2.0094523e-12 1.8597942e-12\n",
      " 6.6448190e-14 1.2319528e-12 2.9934914e-13 7.1204007e-13 4.7551308e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1656828, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1656832, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 60]/5.92sec ============================\n",
      "\u001b[35mTrain accuracy at step 60: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 60: 51.666667%\n",
      "\u001b[31mLoss     value at step 60: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.1924207e-12 1.0000000e+00 5.1489217e-13 2.1884481e-13 3.3472240e-12\n",
      " 3.3759725e-13 8.0968077e-13 2.1590774e-13 1.2016027e-12 1.1317770e-11\n",
      " 3.7596383e-13 2.9026865e-13 2.8485856e-13 2.0146562e-12 1.8643190e-12\n",
      " 6.6672138e-14 1.2345685e-12 3.0023428e-13 7.1415101e-13 4.7666450e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1794841, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1794845, shape=(), dtype=int64, numpy=1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[EPOCH 65]/6.13sec ============================\n",
      "\u001b[35mTrain accuracy at step 65: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 65: 51.666667%\n",
      "\u001b[31mLoss     value at step 65: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.2077778e-12 1.0000000e+00 5.1648952e-13 2.1966196e-13 3.3569423e-12\n",
      " 3.3876344e-13 8.1217715e-13 2.1656765e-13 1.2050225e-12 1.1285091e-11\n",
      " 3.7750442e-13 2.9129916e-13 2.8590148e-13 2.0216389e-12 1.8703882e-12\n",
      " 6.6974970e-14 1.2381010e-12 3.0143235e-13 7.1696533e-13 4.7821258e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=1932854, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=1932858, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 70]/6.63sec ============================\n",
      "\u001b[35mTrain accuracy at step 70: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 70: 51.666667%\n",
      "\u001b[31mLoss     value at step 70: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.2283520e-12 1.0000000e+00 5.1862182e-13 2.2075652e-13 3.3698625e-12\n",
      " 3.4028268e-13 8.1551454e-13 2.1743187e-13 1.2095220e-12 1.1240752e-11\n",
      " 3.7954473e-13 2.9266804e-13 2.8729102e-13 2.0307751e-12 1.8783176e-12\n",
      " 6.7378320e-14 1.2427572e-12 3.0302106e-13 7.2062587e-13 4.8026741e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=2070867, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=2070871, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 75]/6.04sec ============================\n",
      "\u001b[35mTrain accuracy at step 75: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 75: 51.666667%\n",
      "\u001b[31mLoss     value at step 75: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.2568267e-12 1.0000000e+00 5.2154819e-13 2.2226483e-13 3.3875069e-12\n",
      " 3.4232938e-13 8.2010982e-13 2.1861618e-13 1.2156278e-12 1.1183653e-11\n",
      " 3.8231874e-13 2.9454968e-13 2.8919765e-13 2.0428416e-12 1.8888584e-12\n",
      " 6.7932171e-14 1.2491688e-12 3.0518922e-13 7.2555780e-13 4.8306953e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=2208880, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=2208884, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 80]/6.07sec ============================\n",
      "\u001b[35mTrain accuracy at step 80: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 80: 51.666667%\n",
      "\u001b[31mLoss     value at step 80: 310.378632\u001b[0m\n",
      "tf.Tensor(\n",
      "[3.29617258e-12 1.00000000e+00 5.25600613e-13 2.24358699e-13\n",
      " 3.41152388e-12 3.45157794e-13 8.26478968e-13 2.20262746e-13\n",
      " 1.22401763e-12 1.11130575e-11 3.86120741e-13 2.97155556e-13\n",
      " 2.91843020e-13 2.05899707e-12 1.90311373e-12 6.86988511e-14\n",
      " 1.25801095e-12 3.08186528e-13 7.32311998e-13 4.86923273e-13], shape=(20,), dtype=float32)\n",
      "(<tf.Tensor: id=2346893, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=2346897, shape=(), dtype=int64, numpy=1>)\n",
      "\u001b[36m[EPOCH 85]/5.88sec ============================\n",
      "\u001b[35mTrain accuracy at step 85: 49.166667%\n",
      "\u001b[34mEval  accuracy at step 85: 51.666667%\n",
      "\u001b[31mLoss     value at step 85: 310.378632\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ae4e53a452da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4369156b585f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, eval_data, epochs, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4369156b585f>\u001b[0m in \u001b[0;36mgrads\u001b[0;34m(self, x, target, training)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4369156b585f>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, target, training)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4369156b585f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, verbose, steps, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/core.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4292\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4293\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4294\u001b[0;31m         \"transpose_b\", transpose_b)\n\u001b[0m\u001b[1;32m   4295\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4296\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc\u001b[0m in \u001b[0;36m_get_backward_fn\u001b[0;34m(op_name, attrs, num_inputs, op_inputs, op_outputs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_get_backward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morig_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(ds_train, ds_test, epochs=100000, verbose=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2048, input_shape=(20*40,), activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(20, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 2048)              1640448   \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 4,273,684\n",
      "Trainable params: 4,273,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1879, 800)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 18,  2, ...,  9, 11,  6])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1879, 20)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(20)[y_train].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 2/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 3/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 4/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 5/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 6/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 7/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 8/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 9/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 10/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 11/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 12/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 13/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 14/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 15/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 16/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 17/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 18/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 19/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 20/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 21/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 22/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 23/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 24/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 25/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 26/10000\n",
      "1879/1879 [==============================] - 0s 169us/step - loss: 14.1280\n",
      "Epoch 27/10000\n",
      "1879/1879 [==============================] - 0s 158us/step - loss: 14.1280\n",
      "Epoch 28/10000\n",
      "1879/1879 [==============================] - 0s 155us/step - loss: 14.1280\n",
      "Epoch 29/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 30/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 31/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 32/10000\n",
      "1879/1879 [==============================] - 0s 160us/step - loss: 14.1280\n",
      "Epoch 33/10000\n",
      "1879/1879 [==============================] - 0s 156us/step - loss: 14.1280\n",
      "Epoch 34/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 35/10000\n",
      "1879/1879 [==============================] - 0s 154us/step - loss: 14.1280\n",
      "Epoch 36/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 37/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 38/10000\n",
      "1879/1879 [==============================] - 0s 154us/step - loss: 14.1280\n",
      "Epoch 39/10000\n",
      "1879/1879 [==============================] - 0s 162us/step - loss: 14.1280\n",
      "Epoch 40/10000\n",
      "1879/1879 [==============================] - 0s 166us/step - loss: 14.1280\n",
      "Epoch 41/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 42/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 43/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 44/10000\n",
      "1879/1879 [==============================] - 0s 161us/step - loss: 14.1280\n",
      "Epoch 45/10000\n",
      "1879/1879 [==============================] - 0s 157us/step - loss: 14.1280\n",
      "Epoch 46/10000\n",
      "1879/1879 [==============================] - 0s 166us/step - loss: 14.1280\n",
      "Epoch 47/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 48/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 49/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 50/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 51/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 52/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 53/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 54/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 55/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 56/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 57/10000\n",
      "1879/1879 [==============================] - 0s 159us/step - loss: 14.1280\n",
      "Epoch 58/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 59/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 60/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 61/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 62/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 63/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 64/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 65/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 66/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 67/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 68/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 69/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 70/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 71/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 72/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 73/10000\n",
      "1879/1879 [==============================] - 0s 131us/step - loss: 14.1280\n",
      "Epoch 74/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 75/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 76/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 77/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 78/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 79/10000\n",
      "1879/1879 [==============================] - 0s 155us/step - loss: 14.1280\n",
      "Epoch 80/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 81/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 82/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 83/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 84/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 85/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 86/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 87/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 88/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 89/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 90/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 91/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 92/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 93/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 94/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 95/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 96/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 97/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 98/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 99/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 100/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 101/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 102/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 103/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 104/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 105/10000\n",
      "1879/1879 [==============================] - 0s 158us/step - loss: 14.1280\n",
      "Epoch 106/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 107/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 108/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 109/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 110/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 111/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 112/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 113/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 114/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 115/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 116/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 117/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 118/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 119/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 120/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 121/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 122/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 123/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 124/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 125/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 126/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 127/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 128/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 129/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 130/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 131/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 132/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 133/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 134/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 135/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 136/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 137/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 138/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 139/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 140/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 141/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 142/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 143/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 144/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 145/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 146/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 147/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 148/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 149/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 150/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 151/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 152/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 153/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 154/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 155/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 156/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 157/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 158/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 159/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 160/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 161/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 162/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 163/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 164/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 165/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 166/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 167/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 168/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 169/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 170/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 171/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 172/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 173/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 174/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 175/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 176/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 177/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 178/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 179/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 180/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 181/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 182/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 183/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 184/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 185/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 186/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 187/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 188/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 189/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 190/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 191/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 192/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 193/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 194/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 195/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 196/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 197/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 198/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 199/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 200/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 201/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 202/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 203/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 204/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 205/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 206/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 207/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 208/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 209/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 210/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 211/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 212/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 213/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 214/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 215/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 216/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 217/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 218/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 219/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 220/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 221/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 222/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 223/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 224/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 225/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 226/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 227/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 228/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 229/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 230/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 231/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 232/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 233/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 234/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 235/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 236/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 237/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 238/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 239/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 240/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 241/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 242/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 243/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 244/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 245/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 246/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 247/10000\n",
      "1879/1879 [==============================] - 0s 166us/step - loss: 14.1280\n",
      "Epoch 248/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 249/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 250/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 251/10000\n",
      "1879/1879 [==============================] - 0s 131us/step - loss: 14.1280\n",
      "Epoch 252/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 253/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 254/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 255/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 256/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 257/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 258/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 259/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 260/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 261/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 262/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 263/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 264/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 265/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 266/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 267/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 268/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 269/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 270/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 271/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 272/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 273/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 274/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 275/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 276/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 277/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 278/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 279/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 280/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 281/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 282/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 283/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 284/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 285/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 286/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 287/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 288/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 289/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 290/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 291/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 292/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 293/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 294/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 295/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 296/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 297/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 298/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 299/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 300/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 301/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 302/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 303/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 304/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 305/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 306/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 307/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 308/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 309/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 310/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 311/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 312/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 313/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 314/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 315/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 316/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 317/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 318/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 319/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 320/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 321/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 322/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 323/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 324/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 325/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 326/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 327/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 328/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 329/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 330/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 331/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 332/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 333/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 334/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 335/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 336/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 337/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 338/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 339/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 340/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 341/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 342/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 343/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 344/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 345/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 346/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 347/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 348/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 349/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 350/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 351/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 352/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 353/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 354/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 355/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 356/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 357/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 358/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 359/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 360/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 361/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 362/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 363/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 364/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 365/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 366/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 367/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 368/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 369/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 370/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 371/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 372/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 373/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 374/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 375/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 376/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 377/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 378/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 379/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 380/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 381/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 382/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 383/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 384/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 385/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 386/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 387/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 388/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 389/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 390/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 391/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 392/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 393/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 394/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 395/10000\n",
      "1879/1879 [==============================] - 0s 161us/step - loss: 14.1280\n",
      "Epoch 396/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 397/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.12800s - loss: 1\n",
      "Epoch 398/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 399/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 400/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 401/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 402/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 403/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 404/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 405/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 406/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 407/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 408/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 409/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 410/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 411/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 412/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 413/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 414/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 415/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 416/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 417/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 418/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 419/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 420/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 421/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 422/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 423/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 424/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 425/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 426/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 427/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 428/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 429/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 430/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 431/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 432/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 433/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 434/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 435/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 436/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 437/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 438/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 439/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 440/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 441/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 442/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 443/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 444/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 445/10000\n",
      "1879/1879 [==============================] - 0s 131us/step - loss: 14.1280\n",
      "Epoch 446/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 447/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 448/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 449/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 450/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 451/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 452/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 453/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 454/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 455/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 456/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 457/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 458/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 459/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 460/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 461/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 462/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 463/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 464/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 465/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 466/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 467/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 468/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 469/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 470/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 471/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 472/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 473/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 474/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 475/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 476/10000\n",
      "1879/1879 [==============================] - 0s 130us/step - loss: 14.1280\n",
      "Epoch 477/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 478/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 479/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 480/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 481/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 482/10000\n",
      "1879/1879 [==============================] - 0s 130us/step - loss: 14.1280\n",
      "Epoch 483/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 484/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 485/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 486/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 487/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 488/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 489/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 490/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 491/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 492/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 493/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 494/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 495/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 496/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 497/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 498/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 499/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 500/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 501/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 502/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 503/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 504/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 505/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 506/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 507/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 508/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 509/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 510/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 511/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 512/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 513/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 514/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 515/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 516/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 517/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 518/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 519/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 520/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 521/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 522/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 523/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 524/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 525/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 526/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 527/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 528/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 529/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 530/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 531/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 532/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 533/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 534/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 535/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 536/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 537/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 538/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 539/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 540/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 541/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 542/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 543/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 544/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 545/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 546/10000\n",
      "1879/1879 [==============================] - 0s 156us/step - loss: 14.1280\n",
      "Epoch 547/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 548/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 549/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 550/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 551/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 552/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 553/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 554/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 555/10000\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 14.14 - 0s 135us/step - loss: 14.1280\n",
      "Epoch 556/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 557/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 558/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 559/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 560/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 561/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 562/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 563/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 564/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 565/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 566/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 567/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 568/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 569/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 570/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 571/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 572/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 573/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 574/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 575/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 576/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 577/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 578/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 579/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 580/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 581/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 582/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 583/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 584/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 585/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 586/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 587/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 588/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 589/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 590/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 591/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 592/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 593/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 594/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 595/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 596/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 597/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 598/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 599/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 600/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 601/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 602/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 603/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 604/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 605/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 606/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 607/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 608/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 609/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.12800s - loss:\n",
      "Epoch 610/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 611/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 612/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 613/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 614/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 615/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 616/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 617/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 618/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 619/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 620/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 621/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 622/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 623/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 624/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 625/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 626/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 627/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 628/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 629/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 630/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 631/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 632/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 633/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 634/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 635/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 636/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 637/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 638/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 639/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 640/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 641/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 642/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 643/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 644/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 645/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 646/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 647/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 648/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 649/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 650/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 651/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 652/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 653/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 654/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 655/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 656/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 657/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 658/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 659/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 660/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 661/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 662/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 663/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 664/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 665/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 666/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 667/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 668/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 669/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 670/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 671/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 672/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 673/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 674/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 675/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 676/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 677/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 678/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 679/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 680/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 681/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 682/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 683/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 684/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 685/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 686/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 687/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 688/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 689/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 690/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 691/10000\n",
      "1879/1879 [==============================] - 0s 167us/step - loss: 14.1280\n",
      "Epoch 692/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 693/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 694/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 695/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 696/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 697/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 698/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 699/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 700/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 701/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 702/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 703/10000\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 14.16 - 0s 137us/step - loss: 14.1280\n",
      "Epoch 704/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 705/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 706/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 707/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 708/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 709/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 710/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 711/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 712/10000\n",
      "1879/1879 [==============================] - 0s 157us/step - loss: 14.1280\n",
      "Epoch 713/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 714/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 715/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 716/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 717/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 718/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 719/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 720/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 722/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 723/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 724/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 725/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 726/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 727/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 728/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 729/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 730/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 731/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 732/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 733/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 734/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 735/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 736/10000\n",
      "1879/1879 [==============================] - ETA: 0s - loss: 14.20 - 0s 138us/step - loss: 14.1280\n",
      "Epoch 737/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 738/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 739/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 740/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 741/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 742/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 743/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 744/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 745/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 746/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 747/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 748/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 749/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 750/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 751/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 752/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 753/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 754/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 755/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 756/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 757/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 758/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 759/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 760/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 761/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 762/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 763/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 764/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 765/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 766/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 767/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 768/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 769/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 770/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 771/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 772/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 773/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 774/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 775/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 776/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 777/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 778/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 779/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 780/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 781/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 782/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 783/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 784/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 785/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 786/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 787/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 788/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 789/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 790/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 791/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 792/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 793/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 794/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 795/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 796/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 797/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 798/10000\n",
      "1879/1879 [==============================] - 0s 131us/step - loss: 14.1280\n",
      "Epoch 799/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 800/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 801/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 802/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 803/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 804/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 805/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 806/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 807/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 808/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 809/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 810/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 811/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 812/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 813/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 814/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 815/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 816/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 817/10000\n",
      "1879/1879 [==============================] - 0s 134us/step - loss: 14.1280\n",
      "Epoch 818/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 819/10000\n",
      "1879/1879 [==============================] - 0s 133us/step - loss: 14.1280\n",
      "Epoch 820/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 821/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 822/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 823/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 824/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 825/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 826/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 827/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 828/10000\n",
      "1879/1879 [==============================] - 0s 157us/step - loss: 14.1280\n",
      "Epoch 829/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 830/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 831/10000\n",
      "1879/1879 [==============================] - 0s 170us/step - loss: 14.1280\n",
      "Epoch 832/10000\n",
      "1879/1879 [==============================] - 0s 175us/step - loss: 14.1280\n",
      "Epoch 833/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 834/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 835/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 836/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 837/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 838/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 839/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 840/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 841/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 842/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 843/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 844/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 845/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 846/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 847/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 848/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 849/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 850/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 851/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 852/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 853/10000\n",
      "1879/1879 [==============================] - 0s 156us/step - loss: 14.1280\n",
      "Epoch 854/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 855/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 856/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 857/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 858/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 859/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 860/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 861/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 862/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 863/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 864/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 865/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 866/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 867/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 868/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 869/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 870/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 871/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 872/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 873/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 874/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 875/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 876/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 877/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 878/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 879/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 880/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 881/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 882/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 883/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 884/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 885/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 886/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 887/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 888/10000\n",
      "1879/1879 [==============================] - 0s 168us/step - loss: 14.1280\n",
      "Epoch 889/10000\n",
      "1879/1879 [==============================] - 0s 154us/step - loss: 14.1280\n",
      "Epoch 890/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 891/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 892/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 893/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 894/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 895/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 896/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 897/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 898/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 899/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 900/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 901/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 902/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 903/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 904/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 905/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 906/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 907/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 908/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 909/10000\n",
      "1879/1879 [==============================] - 0s 157us/step - loss: 14.1280\n",
      "Epoch 910/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 911/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 912/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 913/10000\n",
      "1879/1879 [==============================] - 0s 161us/step - loss: 14.1280\n",
      "Epoch 914/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 915/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 916/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 917/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 918/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 919/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 920/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 921/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 922/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 923/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 924/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 925/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 926/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 927/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 928/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 929/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 930/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 931/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 932/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 933/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 934/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 935/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 936/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 937/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 938/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 939/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 940/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 941/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 942/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 943/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 944/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 945/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 946/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 947/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 948/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 949/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 950/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 951/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 952/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 953/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 954/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 955/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 956/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 957/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 958/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 959/10000\n",
      "1879/1879 [==============================] - 0s 132us/step - loss: 14.1280\n",
      "Epoch 960/10000\n",
      "1879/1879 [==============================] - 0s 135us/step - loss: 14.1280\n",
      "Epoch 961/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 962/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 963/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 964/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 965/10000\n",
      "1879/1879 [==============================] - 0s 136us/step - loss: 14.1280\n",
      "Epoch 966/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 967/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 968/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 969/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 970/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 971/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 972/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 973/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 974/10000\n",
      "1879/1879 [==============================] - 0s 163us/step - loss: 14.1280\n",
      "Epoch 975/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 976/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 977/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 978/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 979/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 980/10000\n",
      "1879/1879 [==============================] - 0s 142us/step - loss: 14.1280\n",
      "Epoch 981/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 982/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 983/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 984/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 985/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 986/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 987/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 988/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 989/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 990/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 991/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 992/10000\n",
      "1879/1879 [==============================] - 0s 143us/step - loss: 14.1280\n",
      "Epoch 993/10000\n",
      "1879/1879 [==============================] - 0s 156us/step - loss: 14.1280\n",
      "Epoch 994/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 995/10000\n",
      "1879/1879 [==============================] - 0s 148us/step - loss: 14.1280\n",
      "Epoch 996/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 997/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 998/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 999/10000\n",
      "1879/1879 [==============================] - 0s 147us/step - loss: 14.1280\n",
      "Epoch 1000/10000\n",
      "1879/1879 [==============================] - 0s 139us/step - loss: 14.1280\n",
      "Epoch 1001/10000\n",
      "1879/1879 [==============================] - 0s 151us/step - loss: 14.1280\n",
      "Epoch 1002/10000\n",
      "1879/1879 [==============================] - 0s 140us/step - loss: 14.1280\n",
      "Epoch 1003/10000\n",
      "1879/1879 [==============================] - 0s 137us/step - loss: 14.1280\n",
      "Epoch 1004/10000\n",
      "1879/1879 [==============================] - 0s 149us/step - loss: 14.1280\n",
      "Epoch 1005/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 1006/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 1007/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 1008/10000\n",
      "1879/1879 [==============================] - 0s 141us/step - loss: 14.1280\n",
      "Epoch 1009/10000\n",
      "1879/1879 [==============================] - 0s 138us/step - loss: 14.1280\n",
      "Epoch 1010/10000\n",
      "1879/1879 [==============================] - 0s 150us/step - loss: 14.1280\n",
      "Epoch 1011/10000\n",
      "1879/1879 [==============================] - 0s 152us/step - loss: 14.1280\n",
      "Epoch 1012/10000\n",
      "1879/1879 [==============================] - 0s 155us/step - loss: 14.1280\n",
      "Epoch 1013/10000\n",
      "1879/1879 [==============================] - 0s 153us/step - loss: 14.1280\n",
      "Epoch 1014/10000\n",
      "1879/1879 [==============================] - 0s 146us/step - loss: 14.1280\n",
      "Epoch 1015/10000\n",
      "1879/1879 [==============================] - 0s 145us/step - loss: 14.1280\n",
      "Epoch 1016/10000\n",
      "1879/1879 [==============================] - 0s 144us/step - loss: 14.1280\n",
      "Epoch 1017/10000\n",
      " 768/1879 [===========>..................] - ETA: 0s - loss: 14.0194"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-8b3edf3b0dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, np.eye(20)[y_train],epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
